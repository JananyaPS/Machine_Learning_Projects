# Bias & Fairness Auditing for ML Decision Systems

**Production-style pipeline to detect, quantify, and mitigate bias in ML models using industry-standard fairness metrics and reproducible artifacts.**

---

## ğŸ” What problem this solves
Modern ML systems optimize accuracy but can unintentionally produce **systematic disparities** across user groups.  
This project adds a **fairness layer** to the ML lifecycle:

- Audits model outcomes across sensitive attributes (e.g., sex, race)
- Quantifies bias using standard fairness metrics
- Applies mitigation while tracking accuracyâ€“fairness trade-offs

**Scope**: decision models used in personalization, eligibility, and ranking pipelines  
**Focus**: correctness, transparency, and reproducibility (not just accuracy)

---

## ğŸ“Š Key results (example)
- Accuracy / F1 preserved within **<2â€“3%** after mitigation  
- **Demographic Parity Difference â†“** significantly post-mitigation  
- **Equalized Odds gap â†“** with explicit trade-off reporting  

(Exact values logged in `reports/eval_baseline.json` and `reports/mitigation_report.json`)

---

## ğŸ›  Tech stack
- **Python**, scikit-learn  
- **Fairlearn** (fairness metrics)  
- pandas, numpy, matplotlib  
- joblib (model artifacts)  

---

## âš¡ Can I run this in 2 minutes?

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

python src/download_data.py
python src/make_dataset.py
python src/train.py
python src/evaluate.py
python src/mitigate.py --sensitive sex
```
---

## Architecture

The system is designed with a clear **offline training** and **online decisioning** separation, mirroring production ML systems.

```text
OFFLINE (Training & Evaluation)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Logs  â”‚ â†’ â”‚ Preprocess â”‚ â†’ â”‚ Train  â”‚ â†’ â”‚ Evaluate â”‚ â†’ â”‚ Model Registryâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â†“
                                            Fairness Metrics & Thresholds
                                                        â†“
ONLINE (Inference & Decisioning)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Request â”‚ â†’ â”‚ Feature Buildâ”‚ â†’ â”‚ Score  â”‚ â†’ â”‚ Decision â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†‘
               Fairness-aware Thresholds

```
---
## Metrics & Evaluation

### Performance metrics
- **Accuracy**
- **F1 score**

### Fairness metrics (Fairlearn)
- **Demographic Parity Difference**
- **Demographic Parity Ratio**
- **Equalized Odds Difference**
- **Selection Rate** (per group)
- **True Positive Rate (TPR)** and **False Positive Rate (FPR)** (per group)

### Evaluation methodology
- Stratified train/test split to preserve class balance
- Sensitive attributes **excluded from training** and used **only for auditing**
- Group-wise metrics reported alongside global performance
- All metrics saved as versioned JSON artifacts for reproducibility

### Leakage prevention
- No sensitive attributes used as predictive features
- No post-test tuning of thresholds
- Evaluation performed strictly on held-out data

---

## Project Structure

```text
netflix-bias-fairness-ml/
â”œâ”€â”€ README.md                 # project documentation
â”œâ”€â”€ requirements.txt          # python dependencies
â”œâ”€â”€ .gitignore
â”œâ”€â”€ data/                     # auto-created; do not commit raw data
â”œâ”€â”€ models/                   # saved model artifacts
â”œâ”€â”€ reports/                  # metrics (JSON) + plots
â””â”€â”€ src/
    â”œâ”€â”€ config.py             # configuration & hyperparameters
    â”œâ”€â”€ utils.py              # shared utility functions
    â”œâ”€â”€ download_data.py      # dataset download (OpenML)
    â”œâ”€â”€ make_dataset.py       # preprocessing & train/test split
    â”œâ”€â”€ train.py              # baseline model training
    â”œâ”€â”€ evaluate.py           # performance + fairness evaluation
    â””â”€â”€ mitigate.py           # bias mitigation strategy



